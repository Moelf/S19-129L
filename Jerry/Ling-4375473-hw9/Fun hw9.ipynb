{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading DataFrames support into Gadfly.jl\n",
      "└ @ Gadfly /home/akako/.julia/packages/Gadfly/09PWZ/src/mapping.jl:228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reysum (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import StatsBase\n",
    "using DataFrames, Gadfly, CSV, Flux, Cairo\n",
    "\n",
    "function reysum(xs, ys)\n",
    "    local area = 0\n",
    "    for i=2:length(xs)\n",
    "        dx = abs(xs[i] - xs[i-1])\n",
    "        y = (ys[i] + ys[i-1])/2\n",
    "        area += dx*y\n",
    "    end\n",
    "    return round(area, digits=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = CSV.File(\"./data_fourtop.csv\") |> DataFrame\n",
    "# shuffle\n",
    "df = df[StatsBase.shuffle(1:size(df, 1)),:]\n",
    "first(df,3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "First, let's inspect the data to get a feeling of it. From the plots below, we see that there are some\n",
    "separation in nbtags and njets, this is, for sure, because of the physics: our signals are 4-tops and thus we have a bunch of b-quarks coming from the decay.\n",
    "\n",
    "### About Weights\n",
    "From the weight plot we know we're screwed no matter what: the signal has tiny weights, meaning unless we can isolate signal from background completely, the final distribution yield is bound to be *terrible* regardless of our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histnb = plot(df, x=:nbtags, color=:signal, Geom.histogram, style(bar_spacing=1mm),Coord.cartesian(xmin=0.5, xmax=5.5),Scale.color_discrete_hue)\n",
    "histnjets = plot(df, x=:njets, color=:signal, Geom.histogram, style(bar_spacing=1mm), Coord.cartesian(xmin=1.5, xmax=7),Scale.color_discrete_hue)\n",
    "histnb_density = plot(df, x=:nbtags, color=:signal, Geom.histogram, Coord.cartesian(xmin=1, xmax=5),Scale.color_discrete_hue)\n",
    "histnweights = plot(df, x=:weight, color=:signal,Geom.density(), Coord.cartesian(xmin=0, xmax=0.016),Scale.color_discrete_hue)\n",
    "\n",
    "set_default_plot_size(10inch, 6inch)\n",
    "gridstack([histnb histnjets; histnb_density histnweights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We want to sort our data into the form: ( (parameters, signal or not),  .... ). So that we can loop through this matrix, and feed first element to predict(x), and use the signal as loss.\n",
    "\n",
    "Also, we want to **shuffle** (we don't know if it was originally ordered or not), and **separate** them into 2 piles, for validation (underfit? overfit?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% train\n",
    "# 20% test\n",
    "df_train = df[1:round(Int, size(df,1)*0.8),:]\n",
    "df_test = df[round(Int,size(df,1)*0.8+1):end,:]\n",
    "\n",
    "# re-balance train set, match length of signals\n",
    "df_balanced = df_train[df_train.signal .== 1.0,:]\n",
    "bkg_rows = df_train[df_train.signal .== 0.0,:]\n",
    "bkg_rows = bkg_rows[1:size(df_balanced,1),:]\n",
    "append!(df_balanced, bkg_rows);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "df_balanced = df_balanced[StatsBase.shuffle(1:size(df_balanced, 1)),:]\n",
    "\n",
    "xs_train = convert(Matrix, df_balanced[1:end,4:13])\n",
    "ys_train = df_balanced.signal\n",
    "xs_test = convert(Matrix, df_balanced[1:end,4:13])\n",
    "ys_test = df_balanced.signal\n",
    "\n",
    "# make the tag one-hot\n",
    "ys_train = [Flux.onehot(i, [1,0]) for i in ys_train]\n",
    "# zip xs_train and ys_train together\n",
    "data = collect(zip(eachrow(xs_train), ys_train));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression (simplest possible model)\n",
    "#### Ignoreing weight for now. We can use weight when plotting the final yield (which we won't do), and we will see our beautiful model becomes useless once taking weight into interpretation.\n",
    "Basically it's a linear model, where you give the big matrix 10 variable (our observables), then it spits out a number (usually between 0 to 1) that represents the probability for this event being signal.\n",
    "Then you pick a cut, say belowe 0.6 are all bkg, above are all signal, then you have a binary calssifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dense layer is linear, sigma is identity\n",
    "# softmax gives you normalized vector (sort of)\n",
    "m = Chain(Dense(10,2), softmax)\n",
    "\n",
    "# cross entropy is natual choice for onehot data, since we only have 1 or 0, this is basically -p*log(q)\n",
    "loss(x, y) = Flux.crossentropy(m(x), y)\n",
    "\n",
    "# ADAM optimizer https://arxiv.org/abs/1412.6980v8\n",
    "opt = ADAM(0.001, (0.9, 0.999))\n",
    "\n",
    "# make two sets for visualization the train model compare before & after\n",
    "xs_test = convert(Matrix, df_test[1:end,4:13])\n",
    "df_balanced[:model_predict] = [m(xs_train[i,:]).data[1] for i=1:length(xs_train[:,1])]\n",
    "df_test[:model_predict] = [m(xs_test[i,:]).data[1] for i=1:length(xs_test[:,1])]\n",
    "fake_before = plot(df_balanced, x=:model_predict, color=:signal, Guide.Title(\"Train set, untrain\"), Geom.density(bandwidth=0.05))\n",
    "real_before = plot(df_test, x=:model_predict, Guide.Title(\"Test set, untrain\"), color=:signal, Geom.density(bandwidth=0.05));\n",
    "before = hstack(fake_before,real_before)\n",
    "draw(PNG(8inch, 3inch), before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 4 times over our data\n",
    "Flux.@epochs 6 Flux.train!(loss, params(m), data, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced[:model_predict] = [m(xs_train[i,:]).data[1] for i=1:length(xs_train[:,1])]\n",
    "df_test[:model_predict] = [m(xs_test[i,:]).data[1] for i=1:length(xs_test[:,1])]\n",
    "\n",
    "fake_after = plot(df_balanced, x=:model_predict, color=:signal, Guide.Title(\"Train set, trained\"), Geom.density(bandwidth=0.05))\n",
    "real_after = plot(df_test, x=:model_predict, color=:signal, Guide.Title(\"Test set, trained\"), Geom.density(bandwidth=0.05))\n",
    "after = hstack(fake_after,real_after)\n",
    "draw(PNG(8inch, 3inch), after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AUC = DataFrame()\n",
    "# https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "TPR, FPR = [], []\n",
    "TPR2, FPR2 = [], []\n",
    "for i=0:0.001:1\n",
    "    TP  = sum((df_test.signal .== 1.0) .& (df_test.model_predict .> i))\n",
    "    FP = sum((df_test.signal .== 0.0) .& (df_test.model_predict .> i))\n",
    "    TN = sum((df_test.signal .== 0.0) .& (df_test.model_predict .< i))\n",
    "    FN = sum((df_test.signal .== 1.0) .& (df_test.model_predict .< i))\n",
    "    push!(TPR, TP/(TP+FN))\n",
    "    push!(FPR, FP/(FP+TN))\n",
    "    TP2 = sum((df_balanced.signal .== 1.0) .& (df_balanced.model_predict .> i))\n",
    "    FP2 = sum((df_balanced.signal .== 0.0) .& (df_balanced.model_predict .> i))\n",
    "    TN2 = sum((df_balanced.signal .== 0.0) .& (df_balanced.model_predict .< i))\n",
    "    FN2 = sum((df_balanced.signal .== 1.0) .& (df_balanced.model_predict .< i))\n",
    "    push!(TPR2, TP2/(TP2+FN2))\n",
    "    push!(FPR2, FP2/(FP2+TN2))\n",
    "end\n",
    "df_AUC[:FPR] = FPR\n",
    "df_AUC[:TPR] = TPR\n",
    "df_AUC[:FPR2] = FPR2\n",
    "df_AUC[:TPR2] = TPR2\n",
    "test_area = reysum(FPR, TPR)\n",
    "train_area = reysum(FPR2, TPR2)\n",
    "\n",
    "\n",
    "test_ROC = layer(df_AUC, x=:FPR, y=:TPR, Geom.line)\n",
    "train_ROC = layer(df_AUC, x=:FPR2, y=:TPR2, Geom.line, Theme(default_color=\"orange\"))\n",
    "set_default_plot_size(8inch, 5inch)\n",
    "ROC_curve = plot(test_ROC, train_ROC, Guide.Title(\"ROC Curve\"), Guide.manual_color_key(\"Type\", [\"Test area: $(test_area)\", \" Train area: $(train_area)\"], [\"deepskyblue\", \"orange\"]))\n",
    "draw(PNG(6inch, 4inch), ROC_curve);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
